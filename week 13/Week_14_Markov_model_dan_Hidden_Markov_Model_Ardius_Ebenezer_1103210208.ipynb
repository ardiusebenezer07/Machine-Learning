{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xv6ZyRAjBMAi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Generate sample data (replace with your actual data)\n",
        "    np.random.seed(42)\n",
        "    n_samples = 1000\n",
        "    n_features = 10\n",
        "    X = np.random.randn(n_samples, n_features)\n",
        "    y = np.random.randn(n_samples)\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    # Model parameters\n",
        "    hidden_sizes = [32, 64, 128]\n",
        "    pooling_types = ['max', 'avg']\n",
        "    epochs_list = [5, 50, 100, 250, 350]\n",
        "    optimizers = ['sgd', 'rmsprop', 'adam']\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "    sequence_length = 10\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = BankDataset(X_train, y_train, sequence_length)\n",
        "    val_dataset = BankDataset(X_test, y_test, sequence_length)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "    results = []\n",
        "    for hidden_size in hidden_sizes:\n",
        "        for pooling_type in pooling_types:\n",
        "            for num_epochs in epochs_list:\n",
        "                for opt_name in optimizers:\n",
        "                    print(f\"\\nTraining with parameters:\")\n",
        "                    print(f\"Hidden Size: {hidden_size}, Pooling: {pooling_type}\")\n",
        "                    print(f\"Epochs: {num_epochs}, Optimizer: {opt_name}\")\n",
        "\n",
        "                    model = RNNModel(n_features, hidden_size, 1, pooling_type)\n",
        "\n",
        "                    if opt_name == 'sgd':\n",
        "                        optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "                    elif opt_name == 'rmsprop':\n",
        "                        optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
        "                    else:\n",
        "                        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "                    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                        optimizer, mode='min', factor=0.1, patience=3\n",
        "                    )\n",
        "\n",
        "                    criterion = nn.MSELoss()\n",
        "\n",
        "                    train_losses, val_losses = train_model(\n",
        "                        model, train_loader, val_loader,\n",
        "                        criterion, optimizer, scheduler, num_epochs\n",
        "                    )\n",
        "\n",
        "                    # Plot training history\n",
        "                    params = {\n",
        "                        'hidden_size': hidden_size,\n",
        "                        'pooling_type': pooling_type,\n",
        "                        'epochs': num_epochs,\n",
        "                        'optimizer': opt_name\n",
        "                    }\n",
        "                    plot_training_history(train_losses, val_losses, params)\n",
        "\n",
        "                    results.append({\n",
        "                        'hidden_size': hidden_size,\n",
        "                        'pooling_type': pooling_type,\n",
        "                        'epochs': num_epochs,\n",
        "                        'optimizer': opt_name,\n",
        "                        'final_train_loss': train_losses[-1],\n",
        "                        'final_val_loss': val_losses[-1]\n",
        "                    })\n",
        "\n",
        "    # Convert results to DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Analyze and visualize results\n",
        "    analyze_results(results_df)\n",
        "\n",
        "    # Create heatmaps for parameter combinations\n",
        "    param_pairs = [\n",
        "        ('hidden_size', 'epochs'),\n",
        "        ('hidden_size', 'optimizer'),\n",
        "        ('pooling_type', 'optimizer'),\n",
        "        ('epochs', 'optimizer')\n",
        "    ]\n",
        "\n",
        "    for param1, param2 in param_pairs:\n",
        "        plot_results_heatmap(results_df, param1, param2)\n",
        "\n",
        "    # Save results\n",
        "    results_df.to_csv('bank.csv', index=False)\n",
        "    print(\"\\nResults saved to 'bank.csv'\")"
      ],
      "metadata": {
        "id": "z_6ViKVPD1Gh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Previous Dataset and Model classes remain the same as in the first artifact]\n",
        "class BankDataset(Dataset):\n",
        "    def __init__(self, X, y, sequence_length):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.FloatTensor(y)\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.X[idx:idx+self.sequence_length],\n",
        "                self.y[idx+self.sequence_length])"
      ],
      "metadata": {
        "id": "5J7ZytgvCLf1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, pooling_type='max'):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.pooling_type = pooling_type\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, _ = self.rnn(x)\n",
        "        if self.pooling_type == 'max':\n",
        "            pooled = torch.max(output, dim=1)[0]\n",
        "        else:\n",
        "            pooled = torch.mean(output, dim=1)\n",
        "        out = self.fc(pooled)\n",
        "        return out"
      ],
      "metadata": {
        "id": "QIp-ff57CbK0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(train_losses, val_losses, params):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title(f'Training History\\nParams: {params}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_results_heatmap(results_df, x_param, y_param, value='final_val_loss'):\n",
        "    pivot_table = results_df.pivot_table(\n",
        "        values=value,\n",
        "        index=y_param,\n",
        "        columns=x_param,\n",
        "        aggfunc='mean'\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(pivot_table, annot=True, fmt='.4f', cmap='viridis')\n",
        "    plt.title(f'{value} comparison: {y_param} vs {x_param}')\n",
        "    plt.show()\n",
        "\n",
        "def analyze_results(results_df):\n",
        "    # Best configurations\n",
        "    best_config = results_df.loc[results_df['final_val_loss'].idxmin()]\n",
        "    print(\"\\nBest Configuration:\")\n",
        "    for param, value in best_config.items():\n",
        "        print(f\"{param}: {value}\")\n",
        "\n",
        "    # Parameter analysis\n",
        "    params = ['hidden_size', 'pooling_type', 'epochs', 'optimizer']\n",
        "    for param in params:\n",
        "        print(f\"\\nAnalysis by {param}:\")\n",
        "        analysis = results_df.groupby(param)['final_val_loss'].agg(['mean', 'std', 'min', 'max'])\n",
        "        print(analysis)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.boxplot(x=param, y='final_val_loss', data=results_df)\n",
        "        plt.title(f'Loss Distribution by {param}')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "2X62BMsJCbFE"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}