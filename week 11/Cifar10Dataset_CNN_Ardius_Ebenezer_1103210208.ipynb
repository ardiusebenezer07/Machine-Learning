{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "N82cQPz7ylTb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformasi dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load Fashion MNIST\n",
        "train_dataset = datasets.FashionMNIST(\n",
        "    root='datasets/test',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "test_dataset = datasets.FashionMNIST(\n",
        "    root='test',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIsTA_TP5X4v",
        "outputId": "d41b5a7f-1099-4d77-b48c-58d0c2c75ea4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to datasets/test/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 18.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/test/FashionMNIST/raw/train-images-idx3-ubyte.gz to datasets/test/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to datasets/test/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 310kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/test/FashionMNIST/raw/train-labels-idx1-ubyte.gz to datasets/test/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to datasets/test/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.53MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/test/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to datasets/test/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to datasets/test/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 4.80MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/test/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to datasets/test/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to test/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 17.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting test/FashionMNIST/raw/train-images-idx3-ubyte.gz to test/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to test/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 311kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting test/FashionMNIST/raw/train-labels-idx1-ubyte.gz to test/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to test/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.63MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting test/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to test/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to test/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 4.53MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting test/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to test/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, kernel_size=3, pooling_type='max'):\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        # Padding to maintain spatial dimensions\n",
        "        padding = kernel_size // 2\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=kernel_size, padding=padding)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=kernel_size, padding=padding)\n",
        "\n",
        "        # Pooling layer\n",
        "        if pooling_type == 'max':\n",
        "            self.pool = nn.MaxPool2d(2, 2)\n",
        "        else:  # 'avg'\n",
        "            self.pool = nn.AvgPool2d(2, 2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "        # Activation functions\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4afDnIh35X1g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopper:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs,\n",
        "                scheduler=None, early_stopper=None, device='cuda'):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100 * correct / total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_acc = 100 * correct / total\n",
        "\n",
        "        # Update learning rate\n",
        "        if scheduler:\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "        # Early stopping\n",
        "        if early_stopper:\n",
        "            early_stopper(val_loss)\n",
        "            if early_stopper.early_stop:\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        # Record metrics\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}:')\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs"
      ],
      "metadata": {
        "id": "UEl8vpzc5XyE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_models(kernel_sizes, pooling_types, epochs_list, optimizers_list):\n",
        "    # Load and preprocess MNIST dataset\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "    val_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Compare kernel sizes\n",
        "    for kernel_size in kernel_sizes:\n",
        "        model = CNNModel(kernel_size=kernel_size, pooling_type='max').to(device)\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
        "        early_stopper = EarlyStopper(patience=5)\n",
        "\n",
        "        results[f'kernel_{kernel_size}'] = train_model(\n",
        "            model, train_loader, val_loader, optimizer, criterion, 50,\n",
        "            scheduler, early_stopper, device\n",
        "        )\n",
        "\n",
        "    # Compare pooling types\n",
        "    for pooling_type in pooling_types:\n",
        "        model = CNNModel(kernel_size=3, pooling_type=pooling_type).to(device)\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
        "        early_stopper = EarlyStopper(patience=5)\n",
        "\n",
        "        results[f'pooling_{pooling_type}'] = train_model(\n",
        "            model, train_loader, val_loader, optimizer, criterion, 50,\n",
        "            scheduler, early_stopper, device\n",
        "        )\n",
        "\n",
        "    # Compare epochs\n",
        "    for epochs in epochs_list:\n",
        "        model = CNNModel().to(device)\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
        "        early_stopper = EarlyStopper(patience=5)\n",
        "\n",
        "        results[f'epochs_{epochs}'] = train_model(\n",
        "            model, train_loader, val_loader, optimizer, criterion, epochs,\n",
        "            scheduler, early_stopper, device\n",
        "        )\n",
        "\n",
        "    # Compare optimizers\n",
        "    for opt_name in optimizers_list:\n",
        "        model = CNNModel().to(device)\n",
        "\n",
        "        if opt_name == 'SGD':\n",
        "            optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "        elif opt_name == 'RMSprop':\n",
        "            optimizer = optim.RMSprop(model.parameters())\n",
        "        else:  # Adam\n",
        "            optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
        "        early_stopper = EarlyStopper(patience=5)\n",
        "\n",
        "        results[f'optimizer_{opt_name}'] = train_model(\n",
        "            model, train_loader, val_loader, optimizer, criterion, 50,\n",
        "            scheduler, early_stopper, device\n",
        "        )\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "19uWvlRr5XvE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(results):\n",
        "    # Plot comparison results\n",
        "    fig, axes = plt.subplots(4, 2, figsize=(15, 20))\n",
        "\n",
        "    # Plot kernel size comparison\n",
        "    for kernel_size in [3, 5, 7]:\n",
        "        axes[0, 0].plot(results[f'kernel_{kernel_size}'][2], label=f'{kernel_size}x{kernel_size}')\n",
        "    axes[0, 0].set_title('Kernel Size Comparison - Accuracy')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Accuracy (%)')\n",
        "    axes[0, 0].legend()\n",
        "\n",
        "    # Plot pooling comparison\n",
        "    for pooling_type in ['max', 'avg']:\n",
        "        axes[1, 0].plot(results[f'pooling_{pooling_type}'][2], label=pooling_type)\n",
        "    axes[1, 0].set_title('Pooling Type Comparison - Accuracy')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Accuracy (%)')\n",
        "    axes[1, 0].legend()\n",
        "\n",
        "    # Plot epochs comparison\n",
        "    for epochs in [5, 50, 100, 250, 350]:\n",
        "        axes[2, 0].plot(results[f'epochs_{epochs}'][2][:epochs], label=f'{epochs} epochs')\n",
        "    axes[2, 0].set_title('Epochs Comparison - Accuracy')\n",
        "    axes[2, 0].set_xlabel('Epoch')\n",
        "    axes[2, 0].set_ylabel('Accuracy (%)')\n",
        "    axes[2, 0].legend()\n",
        "\n",
        "    # Plot optimizer comparison\n",
        "    for opt_name in ['SGD', 'RMSprop', 'Adam']:\n",
        "        axes[3, 0].plot(results[f'optimizer_{opt_name}'][2], label=opt_name)\n",
        "    axes[3, 0].set_title('Optimizer Comparison - Accuracy')\n",
        "    axes[3, 0].set_xlabel('Epoch')\n",
        "    axes[3, 0].set_ylabel('Accuracy (%)')\n",
        "    axes[3, 0].legend()\n",
        "\n",
        "    # Plot corresponding losses\n",
        "    for kernel_size in [3, 5, 7]:\n",
        "        axes[0, 1].plot(results[f'kernel_{kernel_size}'][0], label=f'{kernel_size}x{kernel_size}')\n",
        "    axes[0, 1].set_title('Kernel Size Comparison - Loss')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].legend()\n",
        "\n",
        "    for pooling_type in ['max', 'avg']:\n",
        "        axes[1, 1].plot(results[f'pooling_{pooling_type}'][0], label=pooling_type)\n",
        "    axes[1, 1].set_title('Pooling Type Comparison - Loss')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Loss')\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "    for epochs in [5, 50, 100, 250, 350]:\n",
        "        axes[2, 1].plot(results[f'epochs_{epochs}'][0][:epochs], label=f'{epochs} epochs')\n",
        "    axes[2, 1].set_title('Epochs Comparison - Loss')\n",
        "    axes[2, 1].set_xlabel('Epoch')\n",
        "    axes[2, 1].set_ylabel('Loss')\n",
        "    axes[2, 1].legend()\n",
        "\n",
        "    for opt_name in ['SGD', 'RMSprop', 'Adam']:\n",
        "        axes[3, 1].plot(results[f'optimizer_{opt_name}'][0], label=opt_name)\n",
        "    axes[3, 1].set_title('Optimizer Comparison - Loss')\n",
        "    axes[3, 1].set_xlabel('Epoch')\n",
        "    axes[3, 1].set_ylabel('Loss')\n",
        "    axes[3, 1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Hf-81R795Xnj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run comparison\n",
        "kernel_sizes = [3, 5, 7]\n",
        "pooling_types = ['max', 'avg']\n",
        "epochs_list = [5, 50, 100, 250, 350]\n",
        "optimizers_list = ['SGD', 'RMSprop', 'Adam']\n",
        "\n",
        "results = compare_models(kernel_sizes, pooling_types, epochs_list, optimizers_list)\n",
        "plot_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmsvyX_561ME",
        "outputId": "8ce828ab-2e40-4e43-ae43-7c41ef3524de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50:\n",
            "Train Loss: 0.2167, Train Acc: 93.36%\n",
            "Val Loss: 0.0468, Val Acc: 98.43%\n",
            "Epoch 2/50:\n",
            "Train Loss: 0.0787, Train Acc: 97.68%\n",
            "Val Loss: 0.0365, Val Acc: 98.85%\n",
            "Epoch 3/50:\n",
            "Train Loss: 0.0631, Train Acc: 98.12%\n",
            "Val Loss: 0.0288, Val Acc: 99.12%\n",
            "Epoch 4/50:\n",
            "Train Loss: 0.0495, Train Acc: 98.57%\n",
            "Val Loss: 0.0235, Val Acc: 99.24%\n",
            "Epoch 5/50:\n",
            "Train Loss: 0.0417, Train Acc: 98.72%\n",
            "Val Loss: 0.0220, Val Acc: 99.25%\n"
          ]
        }
      ]
    }
  ]
}